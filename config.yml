# Grahamwjohnon  
# Created Feb 2023-2025

num_gpus: 2 
 
# DATASET
continue_existing_training: False
cont_train_model_dir: '/media/glommy1/tornados/bse_trained_models/sheldrake_Thu_Apr__3_18_56_55_2025' 
root_save_dir: '/media/glommy1/tornados' # Where the outputs of run will be stored
print_dataset_bargraphs: False # Rough visualization of all data timestamps/types for each patient. Slows down initialization. Typically set to False if not the first run of new model
pat_dir: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/preprocessed_data/Bipole_datasets/By_Channel_Scale/HistEqualScale/data_normalized_to_first_24_hours/wholeband/train_pats' # parent directory where all pats data is stored
pat_num_channels_LUT: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/data/pats_num_channels_LUT.csv'
num_samples: kwargs['num_samples'] = 1024*512 # Within each data pickle file
FS: 512             
atd_file: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/data/all_time_data_01092023_112957.csv'
sleep_file: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/data/all_sleep_stages_06122023.csv'
data_dir_subfolder: '/scaled_data_epochs'
intrapatient_dataset_style: 2 # 1 all data (no SPES), 2 all data (with SPES), 3 ONLY SPES
train_hour_dataset_range: kwargs['train_hour_dataset_range'] = [-1, -1]  # [-1, -1] means all, [X, -1] means from X to end
rand_name_json: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/code2/utilities/animals.json'
num_dataloader_workers: 0 # persistent workers will be True if >0, IMPORTANT: DDP observed to be unstable if != 0
delete_old_checkpoints: True # Will only keep most recent checkpoint  
subprocess_num_threads: 1 

# RANDOM DATA GENERATOR - seperate subprocess
# This utilizes CPU/SSD resources to generate random batches even when plotting/saving etc.
# Default to put them directly into system RAM 
env_python_path: '/home/graham/anaconda3/envs/pytorch_env2/bin/python'
random_gen_script_path: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/code2/gen_random_data.py'
num_buffer_batches: kwargs['num_buffer_batches'] = int(kwargs['train_forward_passes'] / kwargs['num_gpus'])   # 512 # Max files to keep in random generator tmp directry 
num_data_threads: 1 # How many threads to start for each data generator (by each GPU)
nested_max_workers: 8 # For the get_batch subprocess

# POSTERIOR (a.k.a. Encoder)
encode_token_samples: 1 # Token size for encoder transformer
diag_mask_buffer_tokens: 16 # Left/Right how many tokens to mask off diagonal, so total masking is 2x this number along diagonal
padded_channels: 256 # input channels randomly padded up to this number to allow for different number of channels, this is the cross-attention dimension
crattn_num_heads: 8 # padding dimension / h = head dimension (e.g. 256/8 = 32 dimension head)
crattn_num_layers: 16 
crattn_max_seq_len: kwargs['crattn_max_seq_len'] = kwargs['encode_token_samples']
crattn_dropout: 0.1 
transformer_dim: 1024 
transformer_seq_length: 512 # each of these is 'padded_channels' x 'encode_token_samples' worth of data
max_seq_len: kwargs['max_seq_len'] = kwargs['transformer_seq_length']
max_batch_size: kwargs['max_batch_size'] = kwargs['random_pulls_in_batch'] * kwargs['inference_batch_mult']
n_layers: 16
n_heads: 32  # transformer_dim / n_heads = head dimension (e.g. 1024/32 = 32 head dimension)
multiple_of: 256 
ffn_dim_multiplier: 1.0 # Default is 1.0 to make FFN layers account for ~2/3 of total parameter count
attention_dropout: 0.1               
encoder_transformer_activation: "silu"
transformer_start_pos: 0 # Not currently coded up to work if > 0
top_dims: kwargs['top_dims'] = kwargs['transformer_dim'] # kwargs['num_encode_concat_transformer_tokens'] * kwargs['transformer_dim']
hidden_dims: 1024 
latent_dim: 1024 

posterior_mogpredictor_hidden_dim_list: [2048, 1024, 512] # Top should be approximately hidden dim of encoder, bottom bigger than prior_prior_mog_components to prevent bottlenecks
posterior_mogpredictor_dropout: 0.1

mse_weight_min: kwargs['mse_weight_min'] = 3
mse_weight_max: kwargs['mse_weight_max'] = 3
mse_weight_stall_epochs: 0
mse_weight_epochs_TO_max: 0
mse_weight_epochs_AT_max: 99999 # Static weight

LR_max_posterior: kwargs['LR_max_posterior'] = 1e-7 # 5e-7
LR_min_posterior: kwargs['LR_min_posterior'] = 1e-7 # 1e-8 
LR_epochs_stall_posterior: 0
LR_epochs_TO_max_posterior: 2
LR_epochs_AT_max_posterior: 2
manual_gamma_posterior: 0.25 # Will be applied to the LR_max & LR_min every 'manual_step_size'
manual_step_size_posterior: 99999 
posterior_weight_decay: 0.1
adamW_beta1: 0.9
adamW_beta2: 0.95 

# DISCRIMINATOR
disc_hidden_dims: [4096, 2048, 1024, 512, 256]
LR_disc: kwargs['LR_disc'] = 5e-8
discriminator_training_iters: 1  # Discriminator advantage, e.g. 1-5. Higher means more discrinimator iters before being used in GM-VAE adversarial loss
KL_divergence_max_weight: kwargs['KL_divergence_max_weight'] = 0.5
KL_divergence_min_weight: kwargs['KL_divergence_min_weight'] = 0.5
KL_divergence_stall_epochs: 0 # Will keep at KL_divergence_min_weight until stall is complete
KL_divergence_epochs_TO_max: 0 # How fast to anneal in KL weight
KL_divergence_epochs_AT_max: 99999 

# POSTERIOR REGULARIZERS
mean_match_weight_max: kwargs['mean_match_weight_max'] = 1e-2 # 5e-2 # 5e-3 # 5e-3
mean_match_weight_min: kwargs['mean_match_weight_min'] = 1e-2 # 5e-4
mean_match_weight_stall_epochs: 0
mean_match_weight_gamma: 1.0

logvar_match_weight_max: kwargs['logvar_match_weight_max'] = 1e-1 # 1e-2 # 5e-2 # 5e-2
logvar_match_weight_min: kwargs['logvar_match_weight_min'] = 1e-1 # 1e-2 # 5e-3
logvar_match_weight_stall_epochs: 0
logvar_match_weight_gamma: 1.0

gumbel_softmax_temperature_max: 0.05 # Higher means softer assignment to MoG, lower temperature encourages component specialization
gumbel_softmax_temperature_min: 0.05 
gumbel_softmax_temperature_stall_epochs: 0
gumbel_softmax_temperature_gamma: 1.0

posterior_mogpreds_entropy_weight_max: 0.2 # 0.2 # 0.5 # Encourage widespread component use. Titrate just high enough to keep all components alive at beginning of training
posterior_mogpreds_entropy_weight_min: 0.2 # 0.2 # posterior unstable/mode collapse if lowered too much
posterior_mogpreds_entropy_weight_stall_epochs: 0
posterior_mogpreds_entropy_weight_gamma: 0.9

posterior_mogpreds_intersequence_diversity_weight_max: 1 # 0.1 Encourages batchwise diveristy among mogpreds (otherwise entropy will just force uniform distribution)
posterior_mogpreds_intersequence_diversity_weight_min: 0 # 0.1 posterior unstable/mode collapse if lowered too much early on
posterior_mogpreds_intersequence_diversity_weight_stall_epochs: 99999
posterior_mogpreds_intersequence_diversity_gamma: 0.95

# GUASSIAN-PROCESS Posterior Regularizer (Different than Guassian Mixture Prior)
# Encourages token-to-token smoothness in latent space for sequential tokens
gp_sigma: kwargs['gp_sigma'] = int(kwargs['mean_lims'][1] - kwargs['mean_lims'][0] / 4) 
gp_length_scale: kwargs['gp_length_scale'] = int(kwargs['transformer_seq_length'] / 4) # Token-level kernel smoothing
gp_weight_max: kwargs['gp_weight_max'] = 1e-8
gp_weight_min: kwargs['gp_weight_min'] = 0
gp_weight_stall_epochs: 40 # Wait for global latent structure to establish before smoothing small zones temporally
gp_weight_epochs_AT_max: 50 
gp_weight_epochs_TO_max: 25

# DECODER
decoder_base_dims: 4096 # top dim of decoder is [encode_token_samples * decoder_base_dims]
hash_output_range: kwargs['hash_output_range'] = (-5, 5)  # Currently hashing is not being used by decoder, encoder/decoder just 'know' channel order somehow

# PRIOR (a.k.a. Mixture of Guassians, MoG)
prior_mog_components: 8 # Softmax sampling of components with gumbel-softmax trick (noise in softmax modulated by temperature)
mean_lims: [-5, 5] # Make sure the prior's mean/logvars make sense with this limit
logvar_lims: [-5, 1] # Important to cap logvar on high end or KL divergence calculaton itself becomes unstable and will give very negative numbers
prior_initial_mean_spread: 3  # Uniform (e.g. value of 2 will be uniform [-2, 2])
prior_initial_logvar: -2 # will be range [x, 0]. How tight/wide to randomly initialize the prior components

prior_entropy_weight: kwargs['prior_entropy_weight'] = 1e-6 # Helps prevent mode collapse in PRIOR
prior_repulsion_weight: kwargs['prior_repulsion_weight'] = 1e-1 # Helps keep centers of PRIOR guassians far apart, need to scale with number of MoG components to keep loss around 0.2

LR_max_prior: kwargs['LR_max_prior'] = 1e-7
LR_min_prior: kwargs['LR_min_prior'] = 0
LR_epochs_stall_prior: 2 # Don't let it get whipped around at training initiation while model posterior is getting established with baseline regularization (i.e. mean/logvar matching, entropy, diversity)
LR_epochs_TO_max_prior: 1
LR_epochs_AT_max_prior: 99999 # static
manual_gamma_prior: 0.25 # Will be applied to the LR_max & LR_min every 'manual_step_size'
manual_step_size_prior: 99999
prior_weight_decay: 0.1
adamW_beta1_prior: 0.9
adamW_beta2_prior: 0.95

# CLASSIFIER - Adversarial
classifier_hidden_dims: [2048, 1024, 512]
classifier_num_pats: 45 #    ########   Hardcoded  ########
classifier_dropout: 0.1
classifier_weight: 1 # OVERALL ADVERSARIAL training: i.e. reverse gradient (VAE) AND classifier power. # Should probably get into same ballpark as recon and KL_divergence
classifier_alpha_max: kwargs['classifier_alpha_max'] = 1e-1 # Alpha of reverse gradient, will dictate the negative multiplier of gradients that the encoder sees
classifier_alpha_min: kwargs['classifier_alpha_min'] = 0 
classifier_epochs_AT_max: 99999
classifier_epochs_TO_max: 50
classifier_rise_first: True
LR_min_classifier: kwargs['LR_min_classifier'] = 1e-5 # 5e-7
classifier_weight_decay: 0
classifier_adamW_beta1: 0.9
classifier_adamW_beta2: 0.999

# BSE TRAINING 
random_pulls_in_batch: 8 # BATCHSIZE
train_forward_passes: 1024 # Independent of batchsize, just random pulls of data
epochs_to_train: 99999 
train_num_rand_hashes: kwargs['train_num_rand_hashes'] = 2**20  # Number of random channel orders per pat to input into encoder 
# Observed Phenonemon: Position of channels is learned somehow during encoding and embedded diffusely across latent dimensions, 
# Thus, no need for conditional decoding with unique hashing per channel order added to latent vector. Decoder just knows somehow, even with unseen patients. 

# VALIDATION
valunseen_forward_passes: 2048 # Be mindful of data gen file buffer ('num_buffer_batches'), currently only based on train forward passes
val_unseen_hour_dataset_range: kwargs['val_unseen_hour_dataset_range'] = [50, -1]
LR_val_bse: kwargs['LR_val_bse'] = 1e-7 # SHould probably set finetuning LR to about min of training LR if pulsing train LR, or down 1-2 orders of magnitude if train LR is constant (i.e. not pulsing)
LR_val_cls: kwargs['LR_val_cls'] = 1e-4
LR_val_prior: kwargs['LR_val_prior'] = kwargs['LR_val_bse']
LR_val_disc: kwargs['LR_val_disc'] = 1e-5
val_num_rand_hashes: 1 # By having at 1, it limits the number of configs that model needs to finetune for a new patient

# PLOTTING
total_collected_latents: 1024  # Currently just for plotting at the end of an epoch
singlebatch_latent_printing: True
num_singlebatch_channels_recon: 1 # Will be batch * this plotted
singlebatch_printing_interval_train: 256 
singlebatch_printing_interval_val: 64
num_recon_samples: 512 # this will limit how much is shown in recon plots, ERROR if not enough samples, pulls from START and END of transformer sequence 
num_rand_recon_plots: 4
num_singlebatch_dims: 5
num_accumulated_plotting_dims: 5
recent_display_iters: 5 # How often to print to console when training & how often to log to WandB


### ----------------------------------- ###

# BSE INFERENCE - Export Settings
# inference_pat_dir: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/preprocessed_data/Bipole_datasets/By_Channel_Scale/HistEqualScale/data_normalized_to_first_24_hours/wholeband/Mobo_pats' 
# inference_pat_dir: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/preprocessed_data/Bipole_datasets/By_Channel_Scale/HistEqualScale/data_normalized_to_first_24_hours/wholeband/validation_pats'
inference_pat_dir: '/media/glommy1/data/vanderbilt_seeg/Bipole_datasets/By_Channel_Scale/HistEqualScale/data_normalized_to_first_24_hours/wholeband/DurStr_1024s896s_epoched_datasets/train45'
# inference_pat_dir: '/media/glommy1/data/vanderbilt_seeg/Bipole_datasets/By_Channel_Scale/HistEqualScale/data_normalized_to_first_24_hours/wholeband/DurStr_1024s896s_epoched_datasets/val13'
inference_save_dir: '/media/glommy1/tornados/bse_inference/sheldrake_epoch1138_train45_wDecode'
bse_codename: 'sheldrake'
inference_intrapatient_dataset_style: 1 # 1 all data (no SPES), 2 all data (with SPES), 3 ONLY SPES
inference_hour_dataset_range: kwargs['inference_hour_dataset_range'] = [-1, -1] 
inference_num_rand_hashes: kwargs['inference_num_rand_hashes'] = 2**20
inference_forward_passes: 1
inference_batch_mult: 1 # Will enable higher inference throughput, but at cost of more VRAM at all times because max-batch_size needs to be increased for initialization of Transformer
inference_window_sec_list: [64, 64, 16, 16, 1] 
inference_stride_sec_list: [64, 16, 16, 4,  1] # Both window and stride must be multiple of input seconds to encoder,  (i.e. encode_token_samples * transformer_seq_length / FS)

inference_decode: True # if True, will run decode on every single forward pass, save all recon MSE data and plot every 'singlebatch_printing_interval_inference', slower than pure encode
singlebatch_printing_interval_inference: 128
inference_save_svg: True # will save jpg and svg (if false, will only save jpg) larger disk space usage
inference_batch_perpat_override: 1 # Set to -1 for no override. Use with caution, will stop inference after this number of batches per subject

### ----------------------------------- ###

### Brain-State Embedder to Brain-State Predictor (BSE2P) & Brain-State Predictor to Brain-State Embedder (BSP2E) ###


bse2p_chunk_size: kwargs['bse2p_chunk_size'] = kwargs['diag_mask_buffer_tokens'] * 2 * kwargs['transformer_dim']

bse2p_transformer_dim: 64
bse2p_layers: 16
bse2p_num_heads: 8
bse2p_ffn_dim_multiplier: 1.0
bse2p_max_batch_size: kwargs['bse2p_max_batch_size'] = kwargs['bsp_batchsize'] * kwargs['bsp_transformer_seq_length']
bse2p_transformer_seq_length: kwargs['bse2p_transformer_seq_length'] = kwargs['transformer_seq_length']
bse2p_transformer_activation: "silu"

bsp_running_kld_length: 64  ######################################### ####### #### # # # # # # # # # # # # # #  # # # # # # # #  #  #   #  #   #  #      #       #  


### ----------------------------------- ###

### Brain-State Predictor (BSP) ###

continue_existing_training: True
cont_train_model_dir_BSP: '/media/glommy1/tornados/bsp_trained_models/commongonolek_sheldrake_Thu_Jun_19_17_54_44_2025' 

bsp_bse_codename: 'sheldrake' # The BSE that made the embeddings for training
bsp_source_dir: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/preprocessed_data/Bipole_datasets/By_Channel_Scale/HistEqualScale/data_normalized_to_first_24_hours/wholeband/train_pats/*/scaled_data_epochs/all_epochs_woSPES'
bsp_num_dataloader_workers: 0 # observed to be unstable if not set to 0

# BSP Optimizer
bsp_weight_decay: 0.1
bsp_adamW_beta1: 0.9
bsp_adamW_beta2: 0.95 

bsp_singlebatch_latent_printing: True
bsp_singlebatch_printing_interval_train: 128 ##################### 256
bsp_epochs_to_train: 99999

# BSP
bsp_transformer_seq_length: 32
bsp_latent_dim: 1024
bsp_n_heads: 32
bsp_n_layers: 16
bsp_ffn_dim_multiplier: 0.8
bsp_max_batch_size: kwargs['bsp_max_batch_size'] = kwargs['bsp_batchsize']
bsp_max_seq_len: kwargs['bsp_max_seq_len'] = kwargs['bsp_transformer_seq_length']
bsp_transformer_activation: "silu"
bsp_attention_dropout: 0.0

# Discriminator (encourage outputs to be on same manifold as BSE)
bsp_disc_weight: 0

### ----------------------------------- ###
### BSP2E ###

bsp2e_chunk_size: kwargs['bsp2e_chunk_size'] = kwargs['bse2p_chunk_size']

bsp2e_transformer_dim: 64
bsp2e_num_transformer_layers: 16
bsp2e_num_transformer_heads: 8
bsp2e_ffn_dim_multiplier: 1.0
bsp2e_max_batch_size: kwargs['bsp2e_max_batch_size'] = kwargs['bsp_batchsize'] * kwargs['bsp_transformer_seq_length']
bsp2e_transformer_seq_length: kwargs['bsp2e_transformer_seq_length'] = kwargs['transformer_seq_length']
bsp2e_transformer_activation: "silu"


### ----------------------------------- ###
# BSP Training
bsp_LR: kwargs['bsp_LR'] = 5e-6

bsp_loss_weight: 1
seeg_loss_weight: 0

bsp_transformer_start_pos: 0 # hardcode to 0
bsp_epoch_dataset_size: kwargs['bsp_epoch_dataset_size'] = 1024 * kwargs['bsp_batchsize'] ################## 1024 ####################
bsp_batchsize: 1
bsp_recent_display_iters: 5

bsp_anneal_epochs_to_max: 10 
bsp_anneal_epochs_at_max: 10  
bsp_anneal_max_weight: 0.00001 
bsp_anneal_min_weight: 0 

# # BSP GP prior
# bsp_variance: 1.0  # NOT CURRENTLY USED
# bsp_lengthscale: 8.0 # NOT CURRENTLY USED
# bsp_gp_weight: 0 # 0.001  # NOT CURRENTLY USED


### ----------------------------------- ### 

### Brain-State Visualizer (BSV) ###

hard_reset_bsv_opt: False # CAREFUL: if True, will reset BSV weights and optimizer on each restart of script

bsv_dims: kwargs['bsv_dims'] = [1024, 512, 256, 128, kwargs['prior_mog_components']]

bsv_LR: kwargs['bsv_LR'] = 1e-6

bsv_running_kld_length: 64 # Will be multiplied by batchsize

bsv_anneal_epochs_to_max: 5
bsv_anneal_epochs_at_max: 5
bsv_anneal_max_weight: 0.01
bsv_anneal_min_weight: 0

bsv_weight_decay: 0.1
bsv_adamW_beta1: 0.9
bsv_adamW_beta2: 0.95 



### BSV INFERENCE (builds off variables from BSE Inference section)###
models_codename: 'commongonolek_sheldrake'
bsv_inference_save_dir: '/media/glommy1/tornados/bsv_inference/commongonolek_epoch295_sheldrake_epoch1138_train45'