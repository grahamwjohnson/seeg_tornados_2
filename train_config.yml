# Grahamwjohnon

# Dataset formation variables
pat_dir: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/results/Bipole_datasets/By_Channel_Scale/HistEqualScale/data_normalized_to_first_24_hours/wholeband/Mobo_pats' #   2pats'     # 10pats' 
train_val_pat_perc: kwargs['train_val_pat_perc'] = [0.90, 0.10] #[0.5, 0.5] # [0.8, 0.2]
num_freq_bands: 1
pat_num_channels_LUT: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/data/pats_num_channels_LUT.csv'
num_samples: kwargs['num_samples'] = 1024*512 # MATCH TO FILE SELECTION 'duration_stride_str' BELOW 512*512, 10*512 , 60*512    
FS: 512             
atd_file: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/data/all_time_data_01092023_112957.csv'
data_dir_subfolder: '/scaled_data_epochs'
# base_path: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/results'
base_path: '/media/glommy1/model_runs'
scale_type: 'HistEqualScale'  # 'LinearScale', 'HyperTanScaling', 'CubeRootScale', 'HistEqualScale'
channel_scale_style: 'By_Channel_Scale' # 'Same_Scale_For_All_Channels' # By_Channel_Scale # If using multiple freq bands, the preprocessing script rescaales, so be careful and should use 'by channel' scaling 
bipole_or_monopole: 'Bipole_datasets'
scale_epoch_str: 'data_normalized_to_first_24_hours' # 'data_normalized_to_first_24_hours' # 'data_normalized_to_first_seizure_centered_24_hours' # , 'Dictated by preprocessing: Should probably decide to have duration match 'train_hours_desired', but do not HAVE to
# duration_stride_str: 'DurStr_1024s896s_epoched_datasets' # 'DurStr_1024s896s_epoched_datasets', 'DurStr_512s480s_epoched_datasets', 'DurStr_10s2s_epoched_datasets', 'DurStr_60s10s_epoched_datasets'
duration_stride_str: 'Mobo_pats'
freq_bands_str: 'wholeband' # 1to12Hz_12to59Hz_61to179Hz, wholeband
intrapatient_dataset_style: 2 # 1 all data (no SPES), 2 all data (with SPES), 3 ONLY SPES
train_hour_dataset_range: kwargs['train_hour_dataset_range'] = [-1, -1]  # [-1, -1] means all, [X, -1] means from X to end
rand_name_json: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/code2/utilities/animals.json'
periictal_augmentation_perc: 0.0 # May mess with sequential saving of latents if enabled
preictal_augmentation_seconds: 1800 # NOT CURRENTLY BEING USED

# Random data generator (seperate subprocess)
random_gen_script_path: '/media/graham/MOBO_RAID0/Ubuntu_Projects/SEEG_Tornados/code2/gen_random_data.py'
num_buffer_batches: 512 # Max files to keep in random generator tmp directry 
num_data_threads: 1 # How many threads to start for each data generator (by each GPU)
nested_max_workers: 32 # For the get batch subprocess

# Training settings
continue_existing_training: True  # if True and 'cont_train_model_dir' is '', will ask for directory with dialog box 
print_dataset_bargraphs: False # Typically set to False if not the first run of model, but may want true if adding new patients
cont_train_model_dir: '/media/glommy1/model_runs/Bipole_datasets/By_Channel_Scale/HistEqualScale/data_normalized_to_first_24_hours/wholeband/Mobo_pats/trained_models/dataset_train90.0_val10.0/vicuna_Tue_Mar__4_15_39_00_2025' 
inference_selection: 2 # 1: all data w/o SPES, 2: all data w/ SPES, 3: only SPES 

### WAE ###
autoencode_samples: 2 # 8

# Cross atttention embedder
padded_channels: 256 #input padded to allow for different number of channels, also the top level embed dim
crattn_embed_dim: 512  # This times autoencode_samples will be transformer dims e.g. 32*32 = 1024
crattn_num_highdim_heads: 16 # at padding dimension
crattn_num_highdim_layers: 4 # at padding dimension
crattn_num_lowdim_heads: 8 # at embed dim dimension
crattn_num_lowdim_layers: 4 # at embed dim dimension
crattn_max_seq_len: kwargs['crattn_max_seq_len'] = kwargs['autoencode_samples']
crattn_cnn_kernel_size: 3
crattn_dropout: 0.1

### Transformer Encoder
transformer_dim: kwargs['transformer_dim'] = kwargs['crattn_embed_dim'] * kwargs['autoencode_samples']
transformer_seq_length: 512
max_seq_len: kwargs['max_seq_len'] = kwargs['transformer_seq_length']
max_batch_size: kwargs['max_batch_size'] = kwargs['random_pulls_in_batch'] * kwargs['inference_batch_mult']
n_layers: 4
n_heads: 128
multiple_of: 256 
ffn_dim_multiplier: 0.4 # 0.2  # Default is 1 to make FF layers account for 2/3 of total parameter count
attention_dropout: 0.1          # 0.8 is very aggressive     
encoder_transformer_activation: "silu"

### WAE-Layers Encoder
num_encode_concat_transformer_tokens: 8
transformer_start_pos: 0 # Not currently coded up to work if > 0
top_dims: kwargs['top_dims'] = kwargs['num_encode_concat_transformer_tokens'] * kwargs['transformer_dim']
hidden_dims: 4096 # 4096
latent_dim: 2048 # 2048

### DECODER
# Non-autoregressive transformer
decoder_base_dims: 1024 # kwargs['decoder_base_dims'] = kwargs['latent_dim'] # after this layer, it will be connected into FC to [batch, padded_channels * seq_len] to prepare for decoder transformer input

### Classifier
classifier_hidden_dims: [1024, 1024, 512, 256]
classifier_num_pats: 40 #       Hardcoded  ########
classifier_dropout: 0.1

# WAE Optimizer hyperparameters
core_weight_decay: 0.1
adamW_beta1: 0.9
adamW_beta2: 0.95 

# Classifier ptimizer hyperparameters
classifier_weight_decay: 0
classifier_adamW_beta1: 0.9
classifier_adamW_beta2: 0.999

# Patient hash embedding
hash_output_range: kwargs['hash_output_range'] = (0, 2)  # tuple, modify based on approximate range of prior
train_num_rand_hashes: 4096  # Number of random channel orders, with deterministic hash embedding for each
autoreg_num_rand_hashes: 1 # Need to be only one to work? in theory should work with many?
val_num_rand_hashes: 1 # By having at 1, it limits the number of configs that model needs to finetune for a new patient

# Training variables
optimizer_forward_passes: 1 # How often to step optimizier 
random_pulls_in_batch: 8 # Random data pull when pat_curr = -1

train_forward_passes: 1024
valfinetune_forward_passes: 500
valunseen_forward_passes: 500

# VAl and Inference Frequency
inference_every: 99999
inference_batch_mult: 2 # With current architecture/hardware, max batch size > 4 runs out of RAM
finetune_inference: False  
val_every: 99999
num_rand_recon_plots: 4
epochs_to_train: 999999 

# Realtime plotting
realtime_latent_printing: True
num_realtime_channels_recon: 1 # Will be batch * this plotted
realtime_printing_interval: 50
num_recon_samples: 512 # this will limit how much is shown in recon plots, ERROR if not enough samples, pulls from START and END of transformer sequence 
num_realtime_dims: 5
recent_display_iters: 5 # How often to print to console when training & how often to log to WandB


### Dynamic Weights ###

# Beta WAE controls
Reg_max: kwargs['Reg_max'] = 1e0
Reg_min: kwargs['Reg_min'] = 0
Reg_stall_epochs: 0 # No Reg weight rise until stall is complete
Reg_epochs_AT_max: 99999
Reg_epochs_TO_max: 0

# GAMMA distribution params  
running_reg_passes: 8192
barycenter_batch_sampling: 32  # The past samples to incorporate into batch loss
# PRIOR: Multimodal Gamma parameters
multimodal_shapes: [10, 30]
multimodal_scales: [0.05, 0.05]
multimodal_weights: [0.5, 0.5]
num_barycenter_iters: 10
sinkhorn_blur: kwargs['sinkhorn_blur'] = 0.05 # This is the BATCH/SMALL scale blur updates
random_barycenter_numplots: 4
wasserstein_order: 1



# Sparsity weight control
Sparse_max: kwargs['Sparse_max'] = 1e-7 # 1e-6 # 1e-3 #1 #1e-6 # 1e-3  ############# Check if loss is actually used!!! May be commented out in training script ###############
Sparse_min: kwargs['Sparse_min'] = 0
Sparse_epochs_AT_max: 99999
Sparse_epochs_TO_max: 0

# Classifier (Varies alpha of reverse gradient flow, not weight)
classifier_weight: 1 # OVERALL ADVERSARIAL training: i.e. reverse gradient (WAE) AND classifier power. # Should probably get into same ballpark as recon and Reg
classifier_max: kwargs['classifier_max'] = 0 # 1 # 
classifier_min: kwargs['classifier_min'] = 0 # 0.001
classifier_epochs_AT_max: 5
classifier_epochs_TO_max: 5
classifier_rise_first: False

### Static Weights ###
recon_weight: 3 # 3

### Dynamic Learning Rates ###

# WAE learning rate controls
LR_max_core: kwargs['LR_max_core'] = 1e-5 # 5e-7
LR_min_core: kwargs['LR_min_core'] = 1e-5 # 1e-8 
LR_epochs_TO_max_core: 1
LR_epochs_AT_max_core: 1
manual_gamma_core: 0.25 # Will be applied to the LR_max & LR_min every 'manual_step_size'
manual_step_size_core: 99999

# Classifier LR Controls
LR_min_classifier: kwargs['LR_min_classifier'] = 1e-4 # 5e-7
# TODO implement scheduler 

# Val FineTune controls
val_finetune_hour_dataset_range: kwargs['val_finetune_hour_dataset_range'] = [2, 50]
val_unseen_hour_dataset_range: kwargs['val_unseen_hour_dataset_range'] = [50, -1]
epochs_val_finetune: 1
LR_val_wae: kwargs['LR_val_wae'] = 1e-8
LR_val_cls: 1e-4
manual_step_size_val_core: 10
manual_gamma_val_core: 0.5

delete_old_checkpoints: True   

# Previous model initial params
tmp_file_dir: '/tmp_files'

# Multiprocessing 
subprocess_num_threads: 1 
num_dataloader_workers: 0 # persistent workers will be True if >0

# Inference file saves
inference_window_sec_list: [60, 20] # [60, 10]
inference_stride_sec_list: [30, 5] #[30, 5] 


# PaCMAP and HDBSCAN settings
# TODO: Decouple this and leave for manifolds.py
# NN local, MN: global, FP global and local
# Phase 1:
  # w_MN = (1 - itr/phase_1_iters) * w_MN_init + itr/phase_1_iters * 3.0     Starts high, goes down to 3
  # w_neighbors = 2.0
  # w_FP = 1.0
# Phase 2:
  # w_MN = 3.0
  # w_neighbors = 3
  # w_FP = 1
# Phase 3:
  # w_MN = 0.0
  # w_neighbors = 1.
  # w_FP = 1.
# # Work toward seperating out
# apply_pca: True
# pca_comp: 100
# pacmap_build_strs: kwargs['pacmap_build_strs'] = ['train', "valfinetune"]
# pacmap_eval_strs: kwargs['pacmap_eval_strs'] = ["valunseen"]
# pacmap_LR: 1 # 0.5                                                                                              #################
# pacmap_NumIters: kwargs['pacmap_NumIters'] = (300,150,300) # (Phase 1,2,3), default (100, 100, 250) ##################################
# pacmap_NN: kwargs['pacmap_NN'] = None
# pacmap_MN_ratio: 0.5 # 2.0 # 2.0 # Default 0.5
# pacmap_FP_ratio: 2.0 # 0.5 # Default 2.0
# pacmap_MedDim_numdims: 10
# HDBSCAN_min_cluster_size: 200 # e.g. value of 50: 10 seconds avg window with 1 second stride would be ~1 min
# HDBSCAN_min_samples: 100 # Closer to min_cluster_size --> more conservative
# delete_latent_files: False # If false, will not delete the latent files after building pacmap (takes up lots of disk space)


# # Plotting variables
# plot_alpha: 0.5
# plot_preictal_color_sec: 3600
# plot_postictal_color_sec: 3600
# s_plot: 5
# seiz_type_list: kwargs['seiz_type_list'] = ['FBTC', 'FIAS', 'FAS_to_FIAS', 'FAS', 'Focal unknown awareness', 'Unknown', 'Subclinical', 'Non-electrographic'] # Leftward overwites rightward
# seiz_plot_mult: kwargs['seiz_plot_mult'] = [1,       3,     5,              7,    9,                           11,        13,            15] # Assuming increasing order, NOTE: base value of 3 is added in the code
# pic_sub_dirs: kwargs['pic_sub_dirs'] = [] #['MeanLogvar_Train_SEQUENTIAL', 'MeanLogvar_Val_SEQUENTIAL','Recon_Train_SEQUENTIAL' ,'Recon_Val_SEQUENTIAL','Recon_Inference','PaCMAP_EpochWide_Train', 'PaCMAP_EpochWide_Val','PaCMAP_SingleFile_Inference', 'Cluster_Timeline_Train', 'Cluster_Timeline_Val','Cluster_Timeline_Inference'] 
# pic_types: kwargs['pic_types'] = ['JPEGs','SVGs']

